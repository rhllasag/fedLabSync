{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5498d14",
   "metadata": {},
   "source": [
    "#  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d8d9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from argparse import ArgumentParser\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (Input, BatchNormalization, Cropping2D,\n",
    "                                     Concatenate, MaxPooling2D,\n",
    "                                     UpSampling2D, ZeroPadding2D, Lambda,\n",
    "                                     Conv2D, AveragePooling2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c4324",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "## Auxiliar Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6fec3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gathering params\n",
    "dataset_path = \"data/\"\n",
    "number_of_dataset = 4\n",
    "RC=120\n",
    "clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12ae09",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5c5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_RUL(cycle, SoF, EoL, Rc):\n",
    "    if cycle <= SoF:\n",
    "      return Rc\n",
    "    elif SoF <= EoL:\n",
    "      return EoL - cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cff723",
   "metadata": {},
   "source": [
    "## Preprocessing before Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8bb37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Read training data and sort by id and cycle\n",
    "    train_df = pd.read_csv(dataset_path+'train_FD00'+str(number_of_dataset)+'.txt', sep=\" \", header=None)\n",
    "    train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                        's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                        's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "    train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "\n",
    "    # Read testing data - It is the aircraft engine operating data without failure events recorded.\n",
    "    test_df = pd.read_csv(dataset_path+'test_FD00'+str(number_of_dataset)+'.txt', sep=\" \", header=None)\n",
    "    test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                        's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                        's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "     #-------------------DATA PREPROCESSING-----------------------#\n",
    "\n",
    "\n",
    "    rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    #Obtaining data of the first trajectory\n",
    "    first_trajectory=train_df.loc[train_df['id'] == 1]\n",
    "    #Select only the first 32-th cycles to display \n",
    "    altitude = pd.DataFrame(first_trajectory,columns=['cycle','setting1'])\n",
    "    altitude=altitude[:]\n",
    "    mach_number = pd.DataFrame(first_trajectory,columns=['cycle','setting2'])\n",
    "    mach_number=mach_number[:]\n",
    "    throttle_resolver_angle = pd.DataFrame(first_trajectory,columns=['cycle','setting3'])\n",
    "    throttle_resolver_angle=throttle_resolver_angle[:]\n",
    "\n",
    "\n",
    "    # Piece-wise degradation function \n",
    "\n",
    "    # Data Labeling training set - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "    rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'EoL']\n",
    "    rul['Rc']=RC\n",
    "    rul['SoF']=rul['EoL']-rul['Rc']\n",
    "    train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "    train_df['RUL'] = train_df['EoL'] - train_df['cycle']\n",
    "    train_df['RUL'] = train_df.apply(lambda row: add_RUL(row['cycle'], row['SoF'], row['EoL'], row['Rc']), axis=1)\n",
    "    train_df['RUL'] = train_df['RUL'].apply(lambda x: 1 if x >= RC else 0)\n",
    "    train_df.drop('EoL', axis=1, inplace=True)\n",
    "    train_df.drop('SoF', axis=1, inplace=True)\n",
    "    train_df.drop('Rc', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "    truth_df = pd.read_csv(dataset_path+'RUL_FD00'+str(number_of_dataset)+'.txt', sep=\" \", header=None)\n",
    "    truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "    truth_df.columns = ['RUL']\n",
    "\n",
    "    truth_df['id']=truth_df.index +1\n",
    "    test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "\n",
    "    #test_df['RUL'] = test_df['RUL']\n",
    "    max_cycle = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    max_cycle.columns = ['id', 'max_cycle']\n",
    "    test_df = test_df.merge(max_cycle, on=['id'], how='left')\n",
    "    test_df['RUL']= test_df['max_cycle']+test_df['RUL']-test_df['cycle']\n",
    "    test_df.drop('max_cycle', axis=1, inplace=True)\n",
    "    test_df['RUL'] = test_df['RUL'].apply(lambda x: 1 if x >= RC else 0)\n",
    "\n",
    "    # Labeling regimes\n",
    "    kmeans = KMeans(n_clusters=clusters).fit(train_df)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    regime_labels = pd.DataFrame(kmeans.labels_)\n",
    "    train_df['regime']= regime_labels\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=clusters).fit(test_df)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    regime_labels = pd.DataFrame(kmeans.labels_)\n",
    "    test_df['regime']= regime_labels\n",
    "\n",
    "    # Data normalization per regime: Training set\n",
    "\n",
    "    indx = train_df['regime'].copy()\n",
    "    id_ = train_df['id'].copy()\n",
    "    rul_ = train_df['RUL'].copy()\n",
    "    cycle_ = train_df['cycle'].copy()\n",
    "    setting1_ = train_df['setting1'].copy()\n",
    "    setting2_ = train_df['setting2'].copy()\n",
    "    setting3_ = train_df['setting3'].copy()\n",
    "    for indices in train_df.groupby('regime').groups.values():\n",
    "        train_df.loc[indices] = (train_df.loc[indices]-train_df.loc[indices].mean())/train_df.loc[indices].std()\n",
    "    train_df['regime'] = indx\n",
    "    train_df['id'] = id_\n",
    "    train_df['RUL'] = rul_\n",
    "    train_df['cycle'] = cycle_\n",
    "    train_df['setting1'] = setting1_\n",
    "    train_df['setting2'] = setting2_\n",
    "    train_df['setting3'] = setting3_\n",
    "    train_df.drop('regime', axis=1, inplace=True)\n",
    "\n",
    "    # Data normalization per regime: Testing set\n",
    "\n",
    "    indx = test_df['regime'].copy()\n",
    "    id_ = test_df['id'].copy()\n",
    "    rul_ = test_df['RUL'].copy()\n",
    "    cycle_ = test_df['cycle'].copy()\n",
    "    setting1_ = test_df['setting1'].copy()\n",
    "    setting2_ = test_df['setting2'].copy()\n",
    "    setting3_ = test_df['setting3'].copy()\n",
    "    for indices in test_df.groupby('regime').groups.values():\n",
    "        test_df.loc[indices] = (2*(test_df.loc[indices]-test_df.loc[indices].min()))/(test_df.loc[indices].max()-test_df.loc[indices].min())-1\n",
    "    test_df['regime'] = indx\n",
    "    test_df['id'] = id_\n",
    "    test_df['RUL'] = rul_\n",
    "    test_df['cycle'] = cycle_\n",
    "    test_df['setting1'] = setting1_\n",
    "    test_df['setting2'] = setting2_\n",
    "    test_df['setting3'] = setting3_\n",
    "    test_df.drop('regime', axis=1, inplace=True)\n",
    "\n",
    "    # Drop Inf and Nan columns and data normalization: Training set\n",
    "    train_df=train_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "    cols_normalize = train_df.columns.difference(['id','cycle','s1','s2','s3','s4','s5','s6','s7', 's8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21'])\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                                columns=cols_normalize, \n",
    "                                index=train_df.index)\n",
    "    join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df) \n",
    "    train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "    # Drop Inf and Nan columns and data normalization: Testing set\n",
    "    test_df=test_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "    norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                                columns=cols_normalize, \n",
    "                                index=test_df.index)\n",
    "    test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "    test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "    test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13422db3",
   "metadata": {},
   "source": [
    "## Auxiliar Variables Reshaping for U-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2848ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86295a",
   "metadata": {},
   "source": [
    "## Reshaping for U-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a65c755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming columns to training the model \n",
    "sensor_cols = ['s2','s3','s4','s6','s7', 's8','s9','s10','s11','s12','s13','s14','s15','s17','s20','s21']\n",
    "sequence_cols = []\n",
    "sequence_cols.extend(sensor_cols)\n",
    "sequence_length = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a86d4091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sequence X: (39103, 55, 16, 1)\n",
      "Train Sequence Y: (39103, 55, 1)\n",
      "Test Sequence X: (8251, 55, 16, 1)\n",
      "Test Sequence X: (8251, 55, 1)\n"
     ]
    }
   ],
   "source": [
    "            engines = train_df['id'].max()\n",
    "            ids = [*range(1,engines)]\n",
    "            random.shuffle(ids)\n",
    "            training_ids = ids[:int(len(ids)*0.85)] \n",
    "            validation_ids = ids[int(len(ids)*0.85):int(len(ids))] \n",
    "\n",
    "            training_df = train_df.loc[train_df[\"id\"].isin(training_ids)]\n",
    "            validation_df = train_df.loc[train_df[\"id\"].isin(validation_ids)]\n",
    "\n",
    "            # generate sequences X\n",
    "            seq_gen_X_train = (list(gen_sequence(training_df[training_df['id']==id], sequence_length, sequence_cols)) for id in training_df['id'].unique())\n",
    "            seq_gen_X_val = (list(gen_sequence(validation_df[validation_df['id']==id], sequence_length, sequence_cols)) for id in validation_df['id'].unique())\n",
    "\n",
    "            # convert X to numpy array\n",
    "            seq_X_train = np.concatenate(list(seq_gen_X_train)).astype(np.float64)\n",
    "            seq_X_val = np.concatenate(list(seq_gen_X_val)).astype(np.float64)\n",
    "\n",
    "            seq_X_train = seq_X_train.reshape(seq_X_train.shape[0],seq_X_train.shape[1] , seq_X_train.shape[2],1)\n",
    "            seq_X_val = seq_X_val.reshape(seq_X_val.shape[0],seq_X_val.shape[1] , seq_X_val.shape[2],1)\n",
    "            \n",
    "            # generate sequences Y\n",
    "            seq_gen_Y_train = (list(gen_sequence(training_df[training_df['id']==id], sequence_length, ['RUL'])) for id in training_df['id'].unique())\n",
    "            seq_gen_Y_val = (list(gen_sequence(validation_df[validation_df['id']==id], sequence_length, ['RUL'])) for id in validation_df['id'].unique())\n",
    "\n",
    "            # convert Y to numpy array\n",
    "            seq_Y_train = np.concatenate(list(seq_gen_Y_train)).astype(np.float64)\n",
    "            seq_Y_val = np.concatenate(list(seq_gen_Y_val)).astype(np.float64)\n",
    "\n",
    "            seq_Y_train = seq_Y_train.reshape(seq_Y_train.shape[0],seq_Y_train.shape[1] , seq_Y_train.shape[2])\n",
    "            seq_Y_val = seq_Y_val.reshape(seq_Y_val.shape[0],seq_Y_val.shape[1] , seq_Y_val.shape[2])\n",
    "\n",
    "            \n",
    "            print(\"Train Sequence X: \"+str(seq_X_train.shape))\n",
    "            print(\"Train Sequence Y: \"+str(seq_Y_train.shape))\n",
    "            print(\"Test Sequence X: \"+str(seq_X_val.shape))\n",
    "            print(\"Test Sequence X: \"+str(seq_Y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d235c",
   "metadata": {},
   "source": [
    "# Modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6a3a958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(in_,\n",
    "                       depth,\n",
    "                       pools,\n",
    "                       filters,\n",
    "                       kernel_size,\n",
    "                       activation,\n",
    "                       dilation,\n",
    "                       padding,\n",
    "                       complexity_factor,\n",
    "                       regularizer=None,\n",
    "                       name=\"encoder\",\n",
    "                       name_prefix=\"\"):\n",
    "    \n",
    "    \n",
    "        name = \"{}{}\".format(name_prefix, name)\n",
    "        residual_connections = []\n",
    "        for i in range(depth):\n",
    "            l_name = name + \"_L%i\" % i\n",
    "            conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                          activation=activation, padding=padding,\n",
    "                          kernel_regularizer=regularizer,\n",
    "                          bias_regularizer=regularizer,\n",
    "                          dilation_rate=dilation,\n",
    "                          name=l_name + \"_conv1\")(in_)\n",
    "            bn = BatchNormalization(name=l_name + \"_BN1\")(conv)\n",
    "            conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                          activation=activation, padding=padding,\n",
    "                          kernel_regularizer=regularizer,\n",
    "                          bias_regularizer=regularizer,\n",
    "                          dilation_rate=dilation,\n",
    "                          name=l_name + \"_conv2\")(bn)\n",
    "            bn = BatchNormalization(name=l_name + \"_BN2\")(conv)\n",
    "            in_ = MaxPooling2D(pool_size=(pools[i], 1),\n",
    "                               name=l_name + \"_pool\")(bn)\n",
    "\n",
    "            # add bn layer to list for residual conn.\n",
    "            residual_connections.append(bn)\n",
    "            filters = int(filters * 2)\n",
    "\n",
    "        # Bottom\n",
    "        name = \"{}bottom\".format(name_prefix)\n",
    "        conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                      activation=activation, padding=padding,\n",
    "                      kernel_regularizer=regularizer,\n",
    "                      bias_regularizer=regularizer,\n",
    "                      dilation_rate=1,\n",
    "                      name=name + \"_conv1\")(in_)\n",
    "        bn = BatchNormalization(name=name + \"_BN1\")(conv)\n",
    "        conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                      activation=activation, padding=padding,\n",
    "                      kernel_regularizer=regularizer,\n",
    "                      bias_regularizer=regularizer,\n",
    "                      dilation_rate=1,\n",
    "                      name=name + \"_conv2\")(bn)\n",
    "        encoded = BatchNormalization(name=name + \"_BN2\")(conv)\n",
    "\n",
    "        return encoded, residual_connections, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed65b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_modeling(in_=None,\n",
    "                              in_reshaped=None,\n",
    "                              filters=None,\n",
    "                              dense_classifier_activation=None,\n",
    "                              regularizer=None,\n",
    "                              complexity_factor=None,\n",
    "                              name_prefix=\"\",\n",
    "                              n_periods=0,\n",
    "                              input_dims=0,\n",
    "                              n_crops=0,\n",
    "                              **kwargs):\n",
    "        cls = Conv2D(filters=int(filters*complexity_factor),\n",
    "                     kernel_size=(1, 1),\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     bias_regularizer=regularizer,\n",
    "                     activation=dense_classifier_activation,\n",
    "                     name=\"{}dense_classifier_out\".format(name_prefix))(in_)\n",
    "        s = (n_periods * input_dims) - cls.get_shape().as_list()[1]\n",
    "        out = crop_nodes_to_match(\n",
    "            node1=ZeroPadding2D(padding=[[s // 2, s // 2 + s % 2], [0, 0]])(cls),\n",
    "            node2=in_reshaped, n_crops=n_crops\n",
    "        )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f4ddaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_modeling(in_,\n",
    "                            input_dims,\n",
    "                            data_per_period,\n",
    "                            n_periods,\n",
    "                            n_classes,\n",
    "                            transition_window,\n",
    "                            activation,\n",
    "                            regularizer=None,\n",
    "                            name_prefix=\"\"):\n",
    "        cls = AveragePooling2D((data_per_period, 1),\n",
    "                               name=\"{}average_pool\".format(name_prefix))(in_)\n",
    "        out = Conv2D(filters=n_classes,\n",
    "                     kernel_size=(transition_window, 1),\n",
    "                     activation=activation,\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     bias_regularizer=regularizer,\n",
    "                     padding=\"same\",\n",
    "                     name=\"{}sequence_conv_out_1\".format(name_prefix))(cls)\n",
    "        out = Conv2D(filters=n_classes,\n",
    "                     kernel_size=(transition_window, 1),\n",
    "                     activation=\"softmax\",\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     bias_regularizer=regularizer,\n",
    "                     padding=\"same\",\n",
    "                     name=\"{}sequence_conv_out_2\".format(name_prefix))(out)\n",
    "        s = [-1, n_periods, input_dims//data_per_period, n_classes]\n",
    "        if s[2] == 1:\n",
    "            s.pop(2)  # Squeeze the dim\n",
    "        out = Lambda(lambda x: tf.reshape(x, s),\n",
    "                     name=\"{}sequence_classification_reshaped\".format(name_prefix))(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f89f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_upsample(in_,\n",
    "                        res_conns,\n",
    "                        depth,\n",
    "                        pools,\n",
    "                        filters,\n",
    "                        kernel_size,\n",
    "                        activation,\n",
    "                        dilation,  # NOT USED\n",
    "                        padding,\n",
    "                        complexity_factor,\n",
    "                        regularizer=None,\n",
    "                        name=\"upsample\",\n",
    "                        name_prefix=\"\"):\n",
    "        name = \"{}{}\".format(name_prefix, name)\n",
    "        residual_connections = res_conns[::-1]\n",
    "        for i in range(depth):\n",
    "            filters = int(filters/2)\n",
    "            l_name = name + \"_L%i\" % i\n",
    "\n",
    "            # Up-sampling block\n",
    "            fs = pools[::-1][i]\n",
    "            up = UpSampling2D(size=(fs, 1),\n",
    "                              name=l_name + \"_up\")(in_)\n",
    "            conv = Conv2D(int(filters*complexity_factor), (fs, 1),\n",
    "                          activation=activation,\n",
    "                          padding=padding,\n",
    "                          kernel_regularizer=regularizer,\n",
    "                          bias_regularizer=regularizer,\n",
    "                          name=l_name + \"_conv1\")(up)\n",
    "            bn = BatchNormalization(name=l_name + \"_BN1\")(conv)\n",
    "\n",
    "            # Crop and concatenate\n",
    "            cropped_res = crop_nodes_to_match(residual_connections[i], bn, n_crops)\n",
    "            # cropped_res = residual_connections[i]\n",
    "            merge = Concatenate(axis=-1,\n",
    "                                name=l_name + \"_concat\")([cropped_res, bn])\n",
    "            conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                          activation=activation, padding=padding,\n",
    "                          kernel_regularizer=regularizer,\n",
    "                          bias_regularizer=regularizer,\n",
    "                          name=l_name + \"_conv2\")(merge)\n",
    "            bn = BatchNormalization(name=l_name + \"_BN2\")(conv)\n",
    "            conv = Conv2D(int(filters*complexity_factor), (kernel_size, 1),\n",
    "                          activation=activation, padding=padding,\n",
    "                          kernel_regularizer=regularizer,\n",
    "                          bias_regularizer=regularizer,\n",
    "                          name=l_name + \"_conv3\")(bn)\n",
    "            in_ = BatchNormalization(name=l_name + \"_BN3\")(conv)\n",
    "        return in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5082a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def crop_nodes_to_match(node1, node2, n_crops):\n",
    "        \"\"\"\n",
    "        If necessary, applies Cropping2D layer to node1 to match shape of node2\n",
    "        \"\"\"\n",
    "        s1 = np.array(node1.get_shape().as_list())[1:-2]\n",
    "        s2 = np.array(node2.get_shape().as_list())[1:-2]\n",
    "\n",
    "        if np.any(s1 != s2):\n",
    "            n_crops += 1\n",
    "            c = (s1 - s2).astype(np.int)\n",
    "            cr = np.array([c // 2, c // 2]).flatten()\n",
    "            cr[n_crops % 2] += c % 2\n",
    "            cropped_node1 = Cropping2D([list(cr), [0, 0]])(node1)\n",
    "        else:\n",
    "            cropped_node1 = node1\n",
    "        return cropped_node1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194fe8a",
   "metadata": {},
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f45a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psg_utils.utils import ensure_list_or_tuple\n",
    "from typing import List\n",
    "from tensorflow_addons import optimizers as addon_optimizers\n",
    "from tensorflow_addons import activations as addon_activations\n",
    "from tensorflow_addons import losses as addon_losses\n",
    "from tensorflow_addons import metrics as addon_metrics\n",
    "\n",
    "def get_activation_function(activation_string):\n",
    "    \"\"\"\n",
    "    Same as 'init_losses', but for optimizers.\n",
    "    Please refer to the 'init_losses' docstring.\n",
    "    \"\"\"\n",
    "    activation = _get_classes_or_funcs(\n",
    "        activation_string,\n",
    "        func_modules=[tf.keras.activations, addon_activations]\n",
    "    )\n",
    "    assert len(activation) == 1, f'Received unexpected number of activation functions ({len(activation)}, expected 1)'\n",
    "    return activation[0]\n",
    "\n",
    "def _get_classes_or_funcs(string_list: list, func_modules: list) -> List[callable]:\n",
    "    \"\"\"\n",
    "    Helper for 'init_losses' or 'init_metrics'.\n",
    "    Please refer to their docstrings.\n",
    "\n",
    "    Args:\n",
    "        string_list:  (list)   List of strings, each giving a name of a metric\n",
    "                               or loss to use for training. The name should\n",
    "                               refer to a function or class in either tf_funcs\n",
    "                               or custom_funcs modules.\n",
    "        func_modules: (module or list of modules) A Tensorflow.keras module of losses or metrics,\n",
    "                                                  or a list of various modules to look through.\n",
    "\n",
    "    Returns:\n",
    "        A list of len(string_list) of classes/functions of losses/metrics/optimizers/activation functions etc.\n",
    "    \"\"\"\n",
    "    functions_or_classes = []\n",
    "    func_modules = ensure_list_or_tuple(func_modules)\n",
    "    for func_or_class_str in ensure_list_or_tuple(string_list):\n",
    "        found = False\n",
    "        for module in func_modules:\n",
    "            found = getattr(module, func_or_class_str, False)\n",
    "            if found:\n",
    "                print(f\"Found requested class '{func_or_class_str}' in module '{module}'\")\n",
    "                functions_or_classes.append(found)  # return the first found\n",
    "                break\n",
    "        if not found:\n",
    "            raise AttributeError(f\"Did not find loss/metric function {func_or_class_str} \"\n",
    "                                 f\"in the module(s) '{func_modules}'\")\n",
    "    return functions_or_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d5ca6",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "91ef6866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found requested class 'elu' in module '<module 'keras.api._v2.keras.activations' from 'C:\\\\Users\\\\root\\\\.conda\\\\envs\\\\fedLabSync\\\\lib\\\\site-packages\\\\keras\\\\api\\\\_v2\\\\keras\\\\activations\\\\__init__.py'>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\.conda\\envs\\fedLabSync\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparametes\n",
    "n_periods=seq_X_train.shape[1]\n",
    "input_dims=seq_X_train.shape[2]\n",
    "n_channels=1\n",
    "l2_reg = False\n",
    "activation = \"elu\"\n",
    "depth = 4\n",
    "n_crops = 0\n",
    "pools = [8, 6, 4, 2]\n",
    "kernel_size = 5\n",
    "dilation = 2\n",
    "transition_window= 1\n",
    "complexity_factor= 2\n",
    "n_classes= 2\n",
    "init_filters = 4\n",
    "dense_classifier_activation=\"tanh\"\n",
    "transition_window=1\n",
    "padding=\"same\"\n",
    "cf = 2.000\n",
    "name_prefix = \"\"\n",
    "    \n",
    "\n",
    "\n",
    "inputs = Input(shape=[n_periods, input_dims, n_channels])\n",
    "reshaped = [-1, n_periods*input_dims, 1, n_channels]\n",
    "in_reshaped = Lambda(lambda x: tf.reshape(x, reshaped))(inputs) #  shape=(None, 100, 1, 40)\n",
    "\n",
    "# Apply regularization if not None or 0\n",
    "regularizer = regularizers.l2(l2_reg) if l2_reg else None\n",
    "# Get activation func from tf or tfa\n",
    "activation = get_activation_function(activation_string=activation)\n",
    "\n",
    "settings = {\n",
    "            \"depth\": depth,\n",
    "            \"pools\": pools,\n",
    "            \"filters\": init_filters,\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"activation\": activation,\n",
    "            \"dilation\": dilation,\n",
    "            \"padding\": padding,\n",
    "            \"regularizer\": regularizer,\n",
    "            \"name_prefix\": name_prefix,\n",
    "            \"complexity_factor\": cf\n",
    "}\n",
    "\n",
    "fit = {\n",
    "            \"balanced_sampling\": True,\n",
    "            \"use_multiprocessing\": True,\n",
    "            \"channel_mixture\": False,\n",
    "            \"margin\": 17,\n",
    "            \"loss\": 'SparseCategoricalCrossentropy',\n",
    "            \"metrics\": 'SparseCategoricalAccuracy',\n",
    "            \"ignore_out_of_bounds_classes\": True,\n",
    "            \"batch_size\": 12,\n",
    "            \"patience\":8,\n",
    "            \"n_epochs\": 100,\n",
    "            \"verbose\": True,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"optimizer_kwargs\": {\"learning_rate\": 5.0e-06, \"decay\": 0.0, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1.0e-8}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Encoding path\n",
    "\"\"\"\n",
    "enc, residual_cons, filters = create_encoder(in_=in_reshaped,**settings)\n",
    "\n",
    "\"\"\"\n",
    "Decoding path\n",
    "\"\"\"\n",
    "settings[\"filters\"] = filters\n",
    "\n",
    "up = create_upsample(enc, residual_cons, **settings)\n",
    "\n",
    "\"\"\"\n",
    "Dense class modeling layers\n",
    "\"\"\"\n",
    "cls = create_dense_modeling(in_=up,\n",
    "                                         in_reshaped=in_reshaped,\n",
    "                                         filters=n_classes,\n",
    "                                         dense_classifier_activation=dense_classifier_activation,\n",
    "                                         regularizer=regularizer,\n",
    "                                         complexity_factor=cf,\n",
    "                                         name_prefix=name_prefix,\n",
    "                                         n_periods=n_periods,\n",
    "                                         input_dims=input_dims,\n",
    "                                         n_crops=n_crops)\n",
    "\n",
    "\"\"\"\n",
    "Sequence modeling\n",
    "\"\"\"\n",
    "data_per_prediction = input_dims\n",
    "\n",
    "out = create_seq_modeling(in_=cls,\n",
    "                                       input_dims=input_dims,\n",
    "                                       data_per_period=data_per_prediction,\n",
    "                                       n_periods=n_periods,\n",
    "                                       n_classes=n_classes,\n",
    "                                       transition_window=transition_window,\n",
    "                                       activation=activation,\n",
    "                                       regularizer=regularizer,\n",
    "                                       name_prefix=name_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c42ce6",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ae36c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 55, 16, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " lambda_23 (Lambda)             (None, 880, 1, 1)    0           ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " encoder_L0_conv1 (Conv2D)      (None, 880, 1, 8)    48          ['lambda_23[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_L0_BN1 (BatchNormaliza  (None, 880, 1, 8)   32          ['encoder_L0_conv1[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L0_conv2 (Conv2D)      (None, 880, 1, 8)    328         ['encoder_L0_BN1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L0_BN2 (BatchNormaliza  (None, 880, 1, 8)   32          ['encoder_L0_conv2[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L0_pool (MaxPooling2D)  (None, 110, 1, 8)   0           ['encoder_L0_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L1_conv1 (Conv2D)      (None, 110, 1, 16)   656         ['encoder_L0_pool[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_L1_BN1 (BatchNormaliza  (None, 110, 1, 16)  64          ['encoder_L1_conv1[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L1_conv2 (Conv2D)      (None, 110, 1, 16)   1296        ['encoder_L1_BN1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L1_BN2 (BatchNormaliza  (None, 110, 1, 16)  64          ['encoder_L1_conv2[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L1_pool (MaxPooling2D)  (None, 18, 1, 16)   0           ['encoder_L1_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L2_conv1 (Conv2D)      (None, 18, 1, 32)    2592        ['encoder_L1_pool[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_L2_BN1 (BatchNormaliza  (None, 18, 1, 32)   128         ['encoder_L2_conv1[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L2_conv2 (Conv2D)      (None, 18, 1, 32)    5152        ['encoder_L2_BN1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L2_BN2 (BatchNormaliza  (None, 18, 1, 32)   128         ['encoder_L2_conv2[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L2_pool (MaxPooling2D)  (None, 4, 1, 32)    0           ['encoder_L2_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L3_conv1 (Conv2D)      (None, 4, 1, 64)     10304       ['encoder_L2_pool[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_L3_BN1 (BatchNormaliza  (None, 4, 1, 64)    256         ['encoder_L3_conv1[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L3_conv2 (Conv2D)      (None, 4, 1, 64)     20544       ['encoder_L3_BN1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_L3_BN2 (BatchNormaliza  (None, 4, 1, 64)    256         ['encoder_L3_conv2[0][0]']       \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_L3_pool (MaxPooling2D)  (None, 2, 1, 64)    0           ['encoder_L3_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " bottom_conv1 (Conv2D)          (None, 2, 1, 128)    41088       ['encoder_L3_pool[0][0]']        \n",
      "                                                                                                  \n",
      " bottom_BN1 (BatchNormalization  (None, 2, 1, 128)   512         ['bottom_conv1[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bottom_conv2 (Conv2D)          (None, 2, 1, 128)    82048       ['bottom_BN1[0][0]']             \n",
      "                                                                                                  \n",
      " bottom_BN2 (BatchNormalization  (None, 2, 1, 128)   512         ['bottom_conv2[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " upsample_L0_up (UpSampling2D)  (None, 4, 1, 128)    0           ['bottom_BN2[0][0]']             \n",
      "                                                                                                  \n",
      " upsample_L0_conv1 (Conv2D)     (None, 4, 1, 64)     16448       ['upsample_L0_up[0][0]']         \n",
      "                                                                                                  \n",
      " upsample_L0_BN1 (BatchNormaliz  (None, 4, 1, 64)    256         ['upsample_L0_conv1[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L0_concat (Concatenat  (None, 4, 1, 128)   0           ['encoder_L3_BN2[0][0]',         \n",
      " e)                                                               'upsample_L0_BN1[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L0_conv2 (Conv2D)     (None, 4, 1, 64)     41024       ['upsample_L0_concat[0][0]']     \n",
      "                                                                                                  \n",
      " upsample_L0_BN2 (BatchNormaliz  (None, 4, 1, 64)    256         ['upsample_L0_conv2[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L0_conv3 (Conv2D)     (None, 4, 1, 64)     20544       ['upsample_L0_BN2[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L0_BN3 (BatchNormaliz  (None, 4, 1, 64)    256         ['upsample_L0_conv3[0][0]']      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L1_up (UpSampling2D)  (None, 16, 1, 64)    0           ['upsample_L0_BN3[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L1_conv1 (Conv2D)     (None, 16, 1, 32)    8224        ['upsample_L1_up[0][0]']         \n",
      "                                                                                                  \n",
      " cropping2d_23 (Cropping2D)     (None, 16, 1, 32)    0           ['encoder_L2_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " upsample_L1_BN1 (BatchNormaliz  (None, 16, 1, 32)   128         ['upsample_L1_conv1[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L1_concat (Concatenat  (None, 16, 1, 64)   0           ['cropping2d_23[0][0]',          \n",
      " e)                                                               'upsample_L1_BN1[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L1_conv2 (Conv2D)     (None, 16, 1, 32)    10272       ['upsample_L1_concat[0][0]']     \n",
      "                                                                                                  \n",
      " upsample_L1_BN2 (BatchNormaliz  (None, 16, 1, 32)   128         ['upsample_L1_conv2[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L1_conv3 (Conv2D)     (None, 16, 1, 32)    5152        ['upsample_L1_BN2[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L1_BN3 (BatchNormaliz  (None, 16, 1, 32)   128         ['upsample_L1_conv3[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L2_up (UpSampling2D)  (None, 96, 1, 32)    0           ['upsample_L1_BN3[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L2_conv1 (Conv2D)     (None, 96, 1, 16)    3088        ['upsample_L2_up[0][0]']         \n",
      "                                                                                                  \n",
      " cropping2d_24 (Cropping2D)     (None, 96, 1, 16)    0           ['encoder_L1_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " upsample_L2_BN1 (BatchNormaliz  (None, 96, 1, 16)   64          ['upsample_L2_conv1[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L2_concat (Concatenat  (None, 96, 1, 32)   0           ['cropping2d_24[0][0]',          \n",
      " e)                                                               'upsample_L2_BN1[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L2_conv2 (Conv2D)     (None, 96, 1, 16)    2576        ['upsample_L2_concat[0][0]']     \n",
      "                                                                                                  \n",
      " upsample_L2_BN2 (BatchNormaliz  (None, 96, 1, 16)   64          ['upsample_L2_conv2[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L2_conv3 (Conv2D)     (None, 96, 1, 16)    1296        ['upsample_L2_BN2[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L2_BN3 (BatchNormaliz  (None, 96, 1, 16)   64          ['upsample_L2_conv3[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L3_up (UpSampling2D)  (None, 768, 1, 16)   0           ['upsample_L2_BN3[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L3_conv1 (Conv2D)     (None, 768, 1, 8)    1032        ['upsample_L3_up[0][0]']         \n",
      "                                                                                                  \n",
      " cropping2d_25 (Cropping2D)     (None, 768, 1, 8)    0           ['encoder_L0_BN2[0][0]']         \n",
      "                                                                                                  \n",
      " upsample_L3_BN1 (BatchNormaliz  (None, 768, 1, 8)   32          ['upsample_L3_conv1[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L3_concat (Concatenat  (None, 768, 1, 16)  0           ['cropping2d_25[0][0]',          \n",
      " e)                                                               'upsample_L3_BN1[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L3_conv2 (Conv2D)     (None, 768, 1, 8)    648         ['upsample_L3_concat[0][0]']     \n",
      "                                                                                                  \n",
      " upsample_L3_BN2 (BatchNormaliz  (None, 768, 1, 8)   32          ['upsample_L3_conv2[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " upsample_L3_conv3 (Conv2D)     (None, 768, 1, 8)    328         ['upsample_L3_BN2[0][0]']        \n",
      "                                                                                                  \n",
      " upsample_L3_BN3 (BatchNormaliz  (None, 768, 1, 8)   32          ['upsample_L3_conv3[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " dense_classifier_out (Conv2D)  (None, 768, 1, 4)    36          ['upsample_L3_BN3[0][0]']        \n",
      "                                                                                                  \n",
      " zero_padding2d_10 (ZeroPadding  (None, 880, 1, 4)   0           ['dense_classifier_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " average_pool (AveragePooling2D  (None, 55, 1, 4)    0           ['zero_padding2d_10[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequence_conv_out_1 (Conv2D)   (None, 55, 1, 2)     10          ['average_pool[0][0]']           \n",
      "                                                                                                  \n",
      " sequence_conv_out_2 (Conv2D)   (None, 55, 1, 2)     6           ['sequence_conv_out_1[0][0]']    \n",
      "                                                                                                  \n",
      " sequence_classification_reshap  (None, 55, 2)       0           ['sequence_conv_out_2[0][0]']    \n",
      " ed (Lambda)                                                                                      \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 278,164\n",
      "Trainable params: 276,452\n",
      "Non-trainable params: 1,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53913e8d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bbc37439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sequence X: (39103, 55, 16, 1)\n",
      "Train Sequence Y: (39103, 55, 1)\n",
      "Test Sequence X: (8251, 55, 16, 1)\n",
      "Test Sequence X: (8251, 55, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 55, 16, 1), dtype=tf.float64, name=None), TensorSpec(shape=(None, 55, 1), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train Sequence X: \"+str(seq_X_train.shape))\n",
    "print(\"Train Sequence Y: \"+str(seq_Y_train.shape))\n",
    "print(\"Test Sequence X: \"+str(seq_X_val.shape))\n",
    "print(\"Test Sequence X: \"+str(seq_Y_val.shape))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((seq_X_train, seq_Y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(seq_Y_train)).batch(fit['batch_size'])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ef3fe",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 645/3259 [====>.........................] - ETA: 2:10 - loss: 0.5054 - sparse_categorical_accuracy: 0.7528"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "model.compile(fit['optimizer'],loss,metric)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=fit['patience'], restore_best_weights = True)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=fit['n_epochs'], validation_data=(seq_X_val, seq_Y_val), callbacks=[callback], verbose=fit['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff4477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
